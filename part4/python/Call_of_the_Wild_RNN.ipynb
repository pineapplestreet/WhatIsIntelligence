{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/abhaysrivastav/ComputerVision/blob/master/Chararacter_Level_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLtLn8jnu8wS"
   },
   "source": [
    "## Prior Work\n",
    "\n",
    "This notebook is based on work by [abhaysrivastav](https://github.com/abhaysrivastav/ComputerVision). \n",
    "\n",
    "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gk9lV6zu8wb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All of our functions are here in memory....\n"
     ]
    }
   ],
   "source": [
    "# load all libraries and shared functions\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from shared_lstm_functions import *\n",
    "jojo_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QJfnA_beu8wo"
   },
   "source": [
    "## Load in Data\n",
    "\n",
    "We're going to use \"Call of the Wild\" here as our training text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZcat6sSu8ws"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter I. Into the Primitive\\n\\n“Old longings nomadic leap,\\nChafing at custom’s chain;\\nAgain from its'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('/Users/joshpause/Desktop/Experiments/WhatIsIntelligence/part4/data/the_call_of_the_wild.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "text[:100]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aW5XUbQau8w8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 35, 27, 58, 51, 64, 12,  7, 42, 19,  7, 42, 61, 51, 56,  7, 51,\n",
       "       35, 64,  7, 71, 12, 13,  6, 13, 51, 13, 67, 64, 57, 57, 47, 72, 45,\n",
       "       10,  7, 45, 56, 61, 48, 13, 61, 48, 17,  7, 61, 56,  6, 27, 10, 13,\n",
       "       32,  7, 45, 64, 27, 58, 24, 57, 44, 35, 27, 23, 13, 61, 48,  7, 27,\n",
       "       51,  7, 32, 36, 17, 51, 56,  6, 59, 17,  7, 32, 35, 27, 13, 61, 73,\n",
       "       57, 25, 48, 27, 13, 61,  7, 23, 12, 56,  6,  7, 13, 51, 17])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode the text and map each character to an integer and vice versa\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FicPDcILu8yS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[44 35 27 58 51 64 12  7 42 19]\n",
      " [ 7 51 35 64  6 24  7 27 61 10]\n",
      " [11 27 12 10  7 49 45 36 61 10]\n",
      " [27 61 51 64 10  7 13 51 19  7]\n",
      " [32 35 19  7 26 64  7 10 13 10]\n",
      " [17  7 27 61 10  7 27 45 45 24]\n",
      " [ 7 35 13  6 19  7 26 13 17  7]\n",
      " [61 10  7 45 36 12 13 61 48 24]\n",
      " [67 64 61  7 69 27 51 51 35 64]\n",
      " [ 7 51 35 64 57 58 27 12 51 61]]\n",
      "\n",
      "y\n",
      " [[35 27 58 51 64 12  7 42 19  7]\n",
      " [51 35 64  6 24  7 27 61 10  7]\n",
      " [27 12 10  7 49 45 36 61 10 64]\n",
      " [61 51 64 10  7 13 51 19  7 26]\n",
      " [35 19  7 26 64  7 10 13 10  7]\n",
      " [ 7 27 61 10  7 27 45 45 24  7]\n",
      " [35 13  6 19  7 26 13 17  7  6]\n",
      " [10  7 45 36 12 13 61 48 24  7]\n",
      " [64 61  7 69 27 51 51 35 64 11]\n",
      " [51 35 64 57 58 27 12 51 61 64]]\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(encoded, 10, 50)\n",
    "x, y = next(batches)\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aHx2ANO7u8zZ"
   },
   "source": [
    "## Time to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0qiT-SbFu8zd"
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zbk2eCmZu8zo",
    "outputId": "19dbcb2c-2fed-4828-a342-9f0de9ea521d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(74, 82, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=82, out_features=74, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "net = CharRNN(chars, n_hidden=82, n_layers=3, drop_prob=0.5)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbGQ3XeIu8zz",
    "outputId": "c4cbf477-7a94-45fd-b1dc-49292997a00d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Step: 10... Loss: 3.8194... Val Loss: 3.7709\n",
      "Epoch: 2/100... Step: 20... Loss: 3.5944... Val Loss: 3.5870\n",
      "Epoch: 3/100... Step: 30... Loss: 3.5214... Val Loss: 3.5043\n",
      "Epoch: 4/100... Step: 40... Loss: 3.4732... Val Loss: 3.4518\n",
      "Epoch: 5/100... Step: 50... Loss: 3.4542... Val Loss: 3.4238\n",
      "Epoch: 5/100... Step: 60... Loss: 3.4048... Val Loss: 3.3944\n",
      "Epoch: 6/100... Step: 70... Loss: 3.3791... Val Loss: 3.3579\n",
      "Epoch: 7/100... Step: 80... Loss: 3.3432... Val Loss: 3.3380\n",
      "Epoch: 8/100... Step: 90... Loss: 3.3387... Val Loss: 3.3149\n",
      "Epoch: 9/100... Step: 100... Loss: 3.3036... Val Loss: 3.2840\n",
      "Epoch: 10/100... Step: 110... Loss: 3.2876... Val Loss: 3.2427\n",
      "Epoch: 10/100... Step: 120... Loss: 3.2142... Val Loss: 3.1902\n",
      "Epoch: 11/100... Step: 130... Loss: 3.1418... Val Loss: 3.1299\n",
      "Epoch: 12/100... Step: 140... Loss: 3.0859... Val Loss: 3.0826\n",
      "Epoch: 13/100... Step: 150... Loss: 3.0546... Val Loss: 3.0429\n",
      "Epoch: 14/100... Step: 160... Loss: 3.0124... Val Loss: 2.9970\n",
      "Epoch: 15/100... Step: 170... Loss: 2.9998... Val Loss: 2.9582\n",
      "Epoch: 15/100... Step: 180... Loss: 2.9347... Val Loss: 2.9042\n",
      "Epoch: 16/100... Step: 190... Loss: 2.8636... Val Loss: 2.8671\n",
      "Epoch: 17/100... Step: 200... Loss: 2.8206... Val Loss: 2.8109\n",
      "Epoch: 18/100... Step: 210... Loss: 2.7722... Val Loss: 2.7747\n",
      "Epoch: 19/100... Step: 220... Loss: 2.7445... Val Loss: 2.7364\n",
      "Epoch: 20/100... Step: 230... Loss: 2.7294... Val Loss: 2.6994\n",
      "Epoch: 20/100... Step: 240... Loss: 2.7139... Val Loss: 2.6774\n",
      "Epoch: 21/100... Step: 250... Loss: 2.6440... Val Loss: 2.6464\n",
      "Epoch: 22/100... Step: 260... Loss: 2.6104... Val Loss: 2.6157\n",
      "Epoch: 23/100... Step: 270... Loss: 2.5862... Val Loss: 2.6050\n",
      "Epoch: 24/100... Step: 280... Loss: 2.5645... Val Loss: 2.5723\n",
      "Epoch: 25/100... Step: 290... Loss: 2.5880... Val Loss: 2.5562\n",
      "Epoch: 25/100... Step: 300... Loss: 2.5598... Val Loss: 2.5392\n",
      "Epoch: 26/100... Step: 310... Loss: 2.4937... Val Loss: 2.5222\n",
      "Epoch: 27/100... Step: 320... Loss: 2.4903... Val Loss: 2.5051\n",
      "Epoch: 28/100... Step: 330... Loss: 2.4759... Val Loss: 2.4836\n",
      "Epoch: 29/100... Step: 340... Loss: 2.4665... Val Loss: 2.4806\n",
      "Epoch: 30/100... Step: 350... Loss: 2.4856... Val Loss: 2.4607\n",
      "Epoch: 30/100... Step: 360... Loss: 2.4816... Val Loss: 2.4389\n",
      "Epoch: 31/100... Step: 370... Loss: 2.4143... Val Loss: 2.4309\n",
      "Epoch: 32/100... Step: 380... Loss: 2.4114... Val Loss: 2.4116\n",
      "Epoch: 33/100... Step: 390... Loss: 2.3804... Val Loss: 2.4041\n",
      "Epoch: 34/100... Step: 400... Loss: 2.3897... Val Loss: 2.3992\n",
      "Epoch: 35/100... Step: 410... Loss: 2.4175... Val Loss: 2.3825\n",
      "Epoch: 35/100... Step: 420... Loss: 2.4067... Val Loss: 2.3779\n",
      "Epoch: 36/100... Step: 430... Loss: 2.3360... Val Loss: 2.3643\n",
      "Epoch: 37/100... Step: 440... Loss: 2.3381... Val Loss: 2.3623\n",
      "Epoch: 38/100... Step: 450... Loss: 2.3200... Val Loss: 2.3526\n",
      "Epoch: 39/100... Step: 460... Loss: 2.3217... Val Loss: 2.3431\n",
      "Epoch: 40/100... Step: 470... Loss: 2.3519... Val Loss: 2.3179\n",
      "Epoch: 40/100... Step: 480... Loss: 2.3388... Val Loss: 2.3187\n",
      "Epoch: 41/100... Step: 490... Loss: 2.2858... Val Loss: 2.3067\n",
      "Epoch: 42/100... Step: 500... Loss: 2.2924... Val Loss: 2.3029\n",
      "Epoch: 43/100... Step: 510... Loss: 2.2614... Val Loss: 2.2911\n",
      "Epoch: 44/100... Step: 520... Loss: 2.2656... Val Loss: 2.2895\n",
      "Epoch: 45/100... Step: 530... Loss: 2.2985... Val Loss: 2.2853\n",
      "Epoch: 45/100... Step: 540... Loss: 2.2934... Val Loss: 2.2689\n",
      "Epoch: 46/100... Step: 550... Loss: 2.2341... Val Loss: 2.2602\n",
      "Epoch: 47/100... Step: 560... Loss: 2.2321... Val Loss: 2.2691\n",
      "Epoch: 48/100... Step: 570... Loss: 2.2234... Val Loss: 2.2577\n",
      "Epoch: 49/100... Step: 580... Loss: 2.2197... Val Loss: 2.2457\n",
      "Epoch: 50/100... Step: 590... Loss: 2.2704... Val Loss: 2.2345\n",
      "Epoch: 50/100... Step: 600... Loss: 2.2569... Val Loss: 2.2328\n",
      "Epoch: 51/100... Step: 610... Loss: 2.2051... Val Loss: 2.2379\n",
      "Epoch: 52/100... Step: 620... Loss: 2.2150... Val Loss: 2.2325\n",
      "Epoch: 53/100... Step: 630... Loss: 2.1825... Val Loss: 2.2208\n",
      "Epoch: 54/100... Step: 640... Loss: 2.1924... Val Loss: 2.2052\n",
      "Epoch: 55/100... Step: 650... Loss: 2.2169... Val Loss: 2.2240\n",
      "Epoch: 55/100... Step: 660... Loss: 2.2288... Val Loss: 2.2065\n",
      "Epoch: 56/100... Step: 670... Loss: 2.1699... Val Loss: 2.1966\n",
      "Epoch: 57/100... Step: 680... Loss: 2.1760... Val Loss: 2.1940\n",
      "Epoch: 58/100... Step: 690... Loss: 2.1413... Val Loss: 2.1863\n",
      "Epoch: 59/100... Step: 700... Loss: 2.1502... Val Loss: 2.1896\n",
      "Epoch: 60/100... Step: 710... Loss: 2.1895... Val Loss: 2.1784\n",
      "Epoch: 60/100... Step: 720... Loss: 2.1936... Val Loss: 2.1738\n",
      "Epoch: 61/100... Step: 730... Loss: 2.1293... Val Loss: 2.1660\n",
      "Epoch: 62/100... Step: 740... Loss: 2.1377... Val Loss: 2.1634\n",
      "Epoch: 63/100... Step: 750... Loss: 2.1154... Val Loss: 2.1638\n",
      "Epoch: 64/100... Step: 760... Loss: 2.1181... Val Loss: 2.1575\n",
      "Epoch: 65/100... Step: 770... Loss: 2.1611... Val Loss: 2.1463\n",
      "Epoch: 65/100... Step: 780... Loss: 2.1749... Val Loss: 2.1372\n",
      "Epoch: 66/100... Step: 790... Loss: 2.0924... Val Loss: 2.1388\n",
      "Epoch: 67/100... Step: 800... Loss: 2.1059... Val Loss: 2.1380\n",
      "Epoch: 68/100... Step: 810... Loss: 2.0810... Val Loss: 2.1348\n",
      "Epoch: 69/100... Step: 820... Loss: 2.1134... Val Loss: 2.1323\n",
      "Epoch: 70/100... Step: 830... Loss: 2.1267... Val Loss: 2.1278\n",
      "Epoch: 70/100... Step: 840... Loss: 2.1490... Val Loss: 2.1189\n",
      "Epoch: 71/100... Step: 850... Loss: 2.0728... Val Loss: 2.1183\n",
      "Epoch: 72/100... Step: 860... Loss: 2.0821... Val Loss: 2.1283\n",
      "Epoch: 73/100... Step: 870... Loss: 2.0555... Val Loss: 2.1093\n",
      "Epoch: 74/100... Step: 880... Loss: 2.0746... Val Loss: 2.1106\n",
      "Epoch: 75/100... Step: 890... Loss: 2.1138... Val Loss: 2.1122\n",
      "Epoch: 75/100... Step: 900... Loss: 2.1258... Val Loss: 2.1069\n",
      "Epoch: 76/100... Step: 910... Loss: 2.0553... Val Loss: 2.1046\n",
      "Epoch: 77/100... Step: 920... Loss: 2.0701... Val Loss: 2.0970\n",
      "Epoch: 78/100... Step: 930... Loss: 2.0462... Val Loss: 2.0977\n",
      "Epoch: 79/100... Step: 940... Loss: 2.0395... Val Loss: 2.1017\n",
      "Epoch: 80/100... Step: 950... Loss: 2.0907... Val Loss: 2.0930\n",
      "Epoch: 80/100... Step: 960... Loss: 2.1106... Val Loss: 2.0930\n",
      "Epoch: 81/100... Step: 970... Loss: 2.0273... Val Loss: 2.1016\n",
      "Epoch: 82/100... Step: 980... Loss: 2.0389... Val Loss: 2.0943\n",
      "Epoch: 83/100... Step: 990... Loss: 2.0288... Val Loss: 2.0799\n",
      "Epoch: 84/100... Step: 1000... Loss: 2.0364... Val Loss: 2.0913\n",
      "Epoch: 85/100... Step: 1010... Loss: 2.0728... Val Loss: 2.0845\n",
      "Epoch: 85/100... Step: 1020... Loss: 2.0950... Val Loss: 2.0739\n",
      "Epoch: 86/100... Step: 1030... Loss: 2.0102... Val Loss: 2.0756\n",
      "Epoch: 87/100... Step: 1040... Loss: 2.0367... Val Loss: 2.0691\n",
      "Epoch: 88/100... Step: 1050... Loss: 2.0050... Val Loss: 2.0658\n",
      "Epoch: 89/100... Step: 1060... Loss: 2.0241... Val Loss: 2.0697\n",
      "Epoch: 90/100... Step: 1070... Loss: 2.0606... Val Loss: 2.0613\n",
      "Epoch: 90/100... Step: 1080... Loss: 2.0723... Val Loss: 2.0552\n",
      "Epoch: 91/100... Step: 1090... Loss: 1.9926... Val Loss: 2.0620\n",
      "Epoch: 92/100... Step: 1100... Loss: 2.0051... Val Loss: 2.0516\n",
      "Epoch: 93/100... Step: 1110... Loss: 1.9961... Val Loss: 2.0495\n",
      "Epoch: 94/100... Step: 1120... Loss: 1.9999... Val Loss: 2.0576\n",
      "Epoch: 95/100... Step: 1130... Loss: 2.0460... Val Loss: 2.0511\n",
      "Epoch: 95/100... Step: 1140... Loss: 2.0617... Val Loss: 2.0544\n",
      "Epoch: 96/100... Step: 1150... Loss: 1.9823... Val Loss: 2.0551\n",
      "Epoch: 97/100... Step: 1160... Loss: 1.9904... Val Loss: 2.0453\n",
      "Epoch: 98/100... Step: 1170... Loss: 1.9799... Val Loss: 2.0494\n",
      "Epoch: 99/100... Step: 1180... Loss: 1.9902... Val Loss: 2.0483\n",
      "Epoch: 100/100... Step: 1190... Loss: 2.0197... Val Loss: 2.0358\n",
      "Epoch: 100/100... Step: 1200... Loss: 2.0395... Val Loss: 2.0437\n"
     ]
    }
   ],
   "source": [
    "n_seqs = 128 # Number of sequences running through the network in one pass.\n",
    "n_steps = 100 # Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "n_epochs = 100 # Number of epochs\n",
    "train(net, encoded, epochs=n_epochs, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=False, print_every=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "785uFkvgu80D"
   },
   "source": [
    "## Hyperparameters\n",
    "\n",
    "Here are the hyperparameters for the network.\n",
    "\n",
    "In defining the model:\n",
    "* `n_hidden` - The number of units in the hidden layers.\n",
    "* `n_layers` - Number of hidden LSTM layers to use.\n",
    "\n",
    "We assume that dropout probability and learning rate will be kept at the default, in this example.\n",
    "\n",
    "And in training:\n",
    "* `n_seqs` - Number of sequences running through the network in one pass.\n",
    "* `n_steps` - Number of characters in the sequence the network is trained on. Larger is better typically, the network will learn more long range dependencies. But it takes longer to train. 100 is typically a good number here.\n",
    "* `lr` - Learning rate for training\n",
    "\n",
    "Here's some good advice from Andrej Karpathy on training the network. I'm going to copy it in here for your benefit, but also link to [where it originally came from](https://github.com/karpathy/char-rnn#tips-and-tricks).\n",
    "\n",
    "> ## Tips and Tricks\n",
    "\n",
    ">### Monitoring Validation Loss vs. Training Loss\n",
    ">If you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n",
    "\n",
    "> - If your training loss is much lower than validation loss then this means the network might be **overfitting**. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\n",
    "> - If your training/validation loss are about equal then your model is **underfitting**. Increase the size of your model (either number of layers or the raw number of neurons per layer)\n",
    "\n",
    "> ### Approximate number of parameters\n",
    "\n",
    "> The two most important parameters that control the model are `n_hidden` and `n_layers`. I would advise that you always use `n_layers` of either 2/3. The `n_hidden` can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n",
    "\n",
    "> - The number of parameters in your model. This is printed when you start training.\n",
    "> - The size of your dataset. 1MB file is approximately 1 million characters.\n",
    "\n",
    ">These two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n",
    "\n",
    "> - I have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make `n_hidden` larger.\n",
    "> - I have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\n",
    "\n",
    "> ### Best models strategy\n",
    "\n",
    ">The winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n",
    "\n",
    ">It is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n",
    "\n",
    ">By the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D88yJ4RMu80K"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_name = 'cotw_rnn_v25.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open('models/'+model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166862"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Chararacter-Level RNN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
